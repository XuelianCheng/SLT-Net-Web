<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>SLT-Net</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Creative and Descriptive Paper Title." />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">Implicit Motion Handling for Video Camouflaged Object Detection</span>
		<table align=center width=1000px>
			<table align=center width=1000px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:18px"><a href="https://scholar.google.com/citations?user=gQ5kHH8AAAAJ&hl=en">Xuelian Cheng</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:18px"><a href="https://scholar.google.com/citations?user=l4hm14MAAAAJ&hl=en">Huan Xiong</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:18px"><a href="https://dengpingfan.github.io/">Deng-Ping Fan</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:18px"><a href="https://scholar.google.com.sg/citations?user=E9NVOBUAAAAJ&hl=en">Yiran Zhong</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:18px"><a href="https://sites.google.com/site/mehrtashharandi/">Mehrtash Harandi</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:18px"><a href="http://twd20g.blogspot.com/">Tom Drummond</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:18px"><a href="https://zongyuange.github.io/">Zongyuan Ge</a></span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=800>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:20px"><a href='https://arxiv.org/abs/2203.07363'>[Paper]</a></span>
						</center>
					</td>					
					<td align=center width=120px>
						<center>
							<span style="font-size:20px"><a href='paper/[2022][CVPR]VCOD_MoCA-Mask_Chinese.pdf'>[中译版]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:20px"><a href='https://github.com/XuelianCheng/SLT-Net'>[GitHub]</a></span><br>
						</center>
					</td>					
				</tr>
			</table>
			<table align=center width=800>
				<tr>				
					<td align=center width=250px>
						<center>
							<span style="font-size:20px">MoCA-Mask:</span>
							<span style="font-size:20px"><a href='https://drive.google.com/file/d/1FB24BGVrPOeUpmYbKZJYL5ermqUvBo_6/view?usp=sharing'>[Google Drive]</a></span>
							<span style="font-size:20px"><a href='https://pan.baidu.com/s/1IT9nQT5xdX0zWvRVH5WDmQ?pwd=dfpo'>[Baidu]</a></span><br>
						</center>
					</td>
					<td align=center width=300px>
						<center>						
							<span style="font-size:20px">MoCA-Mask-Pseudo:</span>
							<span style="font-size:20px"><a href='https://drive.google.com/file/d/1a7ESYE30q5MHsmrfTasNEOsbjdpZdgz5/view?usp=sharing'>[Google Drive]</a></span>
							<span style="font-size:20px"><a href='https://pan.baidu.com/s/1sBOc9k8KOOG1gHpFms4Aig?pwd=6llz'>[Baidu]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				We propose a new video camouflaged object detection (VCOD) framework that can exploit both short-term dynamics and long-term temporal consistency to detect camouflaged 
				objects from video frames. An essential property of camouflaged objects is that they usually exhibit patterns 
				similar to the background and thus make them hard to identify from still images. Therefore, effectively handling
				temporal dynamics in videos becomes the key for the VCOD task as the camouflaged objects will be noticeable 
				when they move. However, current VCOD methods often leverage homography or optical flows to represent 
				motions, where the detection error may accumulate from both the motion estimation error and the segmentation error.
				On the other hand, our method unifies motion estimation and object segmentation within a single optimization 
				framework. Specifically, we build a dense correlation volume to implicitly capture motions between neighbouring 
				frames and utilize the final segmentation supervision to optimize the implicit motion estimation and segmentation 
				jointly. Furthermore, to enforce temporal consistency within a video sequence, we jointly utilize a spatio-temporal 
				transformer to refine the short-term predictions. Extensive experiments on VCOD benchmarks demonstrate the architectural effectiveness of our approach.
			</td>
		</tr>
	</table>
	<br>

	<center><h1>Overall Pipeline</h1></center>
	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:550px" src="./images/overall.png"/>
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
				The overall pipeline of the SLT-Net. The SLT-Net consists of a short-term detection module and a long-term refinement module.
				The short-term detection module takes a pair of consecutive frames and predicts the camouflaged object mask for the reference 
				frame. The long-term refinement module takes T predictions from the short-term detection module along with their corresponding 
				referenced frames to generate the final predictions.	
				</td>
			</tr>
		</table>
	</center>
	
	<center><h1>Short-term Pipeline</h1></center>
	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:600px" src="./images/short_term.png"/>
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
				The overview of our short-term pipeline. The network first extracts features from the input frames by a transformer encoder,
				then computes a full-range volumetric correspondence between the reference frame I_t and its neighboring frame I_{t+1} to form a correlation
				volume pyramid. A CNN decoder is used to predict the final prediction from the motions captured by the short-term correlation pyramid.
				</td>
			</tr>
		</table>
	</center>

	<center><h1>Long-term Pipeline</h1></center>
	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:600px" src="./images/long_term.png"/>
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
				The overview of the proposed long-term consistency architecture. It formulates the process as a seq-to-seq modeling problem
				and refines the pair-wise predictions with a sequence-to-sequence transformer.
				</td>
			</tr>
		</table>
	</center>

	<br>
	<hr>
	<center><h1>MoCA-Mask Dataset</h1></center>
	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:600px" src="./images/summ.png"/>
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
					Summary for training and test set distribution. Our MoCA-Mask dataset includes 87 video sequences in total, 
					in which 16 sequences were tagged as ''unknow'' (colored in orange). 
					This split is used to validate the sensitivity of different models on novel samples. Zoom-in for details.
				</td>
			</tr>
		</table>
	</center>

	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:600px" src="./images/samples.png"/>
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
					Representative samples from MoCA-Mask. The dataset is quite challenging including diverse scenes, suash as various lighting conditions, 
					i.e, dark and sunny, complex background, camera motions, small ratio of animals and tiny body structures, such as slim torso /limbs.
				</td>
			</tr>
		</table>
	</center>

	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:600px" src="./images/supp.png"/>
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
				Comparison of our proposed network with two top-performing baselines on MoCA-Mask test dataset.
				</td>
			</tr>
		</table>
	</center>

	<br>

	<table align=center width=450px>
		<center><h1>Paper and Supplementary Material</h1></center>
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></a></td>
			<td><span style="font-size:14pt">Xuelian Cheng, Huan Xiong, Deng-Ping Fan, Yiran Zhong, Mehrtash Harandi, Tom Drummond, Zongyuan Ge<br>
				<b>Implicit Motion Handling for Video Camouflaged Object Detection</b><br>
				In CVPR, 2022.<br>
				(hosted on <a href="http://arxiv.org/abs/2203.07363">ArXiv</a>)<br>
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="./resources/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>

	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>
	
	<hr>	
	<br>
	<table align=center width=400>
		<tr>
			<td width=200><center><h1>Visitors</h1></center>
				<!-- <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=uImnlGQbvLcGtZlwHXUkdnb_6p8QDj4V_CPra0MAdRM"></script> -->
				<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=uImnlGQbvLcGtZlwHXUkdnb_6p8QDj4V_CPra0MAdRM&co=1e5882&cmo=5daa5f&cmn=7f313b&ct=ffffff'></script>
			</td>
		</tr>
	</table>

<br>
</body>
</html>