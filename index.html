<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>SLT-Net</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Creative and Descriptive Paper Title." />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<div class="github_logo w-embed">
		<a href="https://github.com/XuelianCheng/SLT-Net" class="github-corner" aria-label="View source on Github">
			<svg width="80" height="80" viewBox="0 0 250 250" style="fill:#333333; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true">
				<defs>
					<mask id="octomask">
					<path fill="white" d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
					<path fill="black" d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" style="transform-origin: 130px 106px;" class="octo-arm"></path>
					<path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="black" class="octo-body"></path></mask>
				</defs>
				<rect class="filler" width="100%" height="100%" mask="url(#octomask)"></rect>
			</svg>
		</a>
		<style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
	</div>
	<div class="section hero nerf-_v2 wf-section">
		<div class="container-2 nerf_header_v2 w-container">
			<h1 class="nerf_title_v2">SLT-Net</h1>
			<h1 class="nerf_subheader_v2">Implicit Motion Handling for Video Camouflaged Object Detection</h1>
			<h1 class="eccv_label">CVPR 2022</h1>
			<div class="nerf_authors_list_single w-row">
				<div class="w-col w-col-2 w-col-small-4 w-col-tiny-6">
					<a href="https://xueliancheng.github.io/" target="_blank" class="nerf_authors_v2">Xuelian Cheng<span class="text-span_nerf_star">*</span><span class="superscript text-span_nerf">1*</span></a>
				</div>
				<div class="column w-col w-col-2 w-col-small-4 w-col-tiny-6">
					<a href="https://scholar.google.com/citations?user=l4hm14MAAAAJ&hl=en" target="_blank" class="nerf_authors_v2">Huan Xiong<span class="text-span_nerf_star">3</span><span class="superscript text-span_nerf"></span></a>
				</div>
				<div class="w-col w-col-2 w-col-small-4 w-col-tiny-6">
					<a href="https://dengpingfan.github.io/" target="_blank" class="nerf_authors_v2">Deng-Ping Fan<span class="text-span_nerf_star">*</span><span class="superscript text-span_nerf">3</span></a>
				</div>
				<div class="w-col w-col-2 w-col-small-4 w-col-tiny-6">
					<a href="https://scholar.google.com.sg/citations?user=E9NVOBUAAAAJ&hl=en" target="_blank" class="nerf_authors_v2">Yiran Zhong<span class="text-span_nerf">4</span><span class="superscript"></span></a>
				</div>
				<div class="w-col w-col-2 w-col-small-4 w-col-tiny-6">
					<a href="https://sites.google.com/site/mehrtashharandi/" target="_blank" class="nerf_authors_v2">Mehrtash Harandi<span class="text-span_nerf">1</span></a>
				</div>
				<div class="w-col w-col-2 w-col-small-4 w-col-tiny-6">
					<a href="http://twd20g.blogspot.com/" target="_blank" class="nerf_authors_v2">Tom Drummond<span class="text-span_nerf">5</span></a>
				</div>
				<div class="w-col w-col-2 w-col-small-4 w-col-tiny-6">
					<a href="https://zongyuange.github.io/" target="_blank" class="nerf_authors_v2">Zongyuan Ge<span class="text-span_nerf">1</span></a>
				</div>
			</div>
			<div class="columns-6 w-row">
				<div class="nerf_mobile_col_inst w-col w-col-4 w-col-small-4 w-col-tiny-4">
					<div class="nerf_mobile_inst"><span class="text-span_nerf">1 </span>Monash University</div>
				</div>
				<div class="nerf_mobile_col_inst w-col w-col-4 w-col-small-4 w-col-tiny-4">
					<div class="nerf_mobile_inst"><span class="text-span_nerf">2 </span> MBZUAI </div>
				</div>
				<div class="nerf_mobile_col_inst w-col w-col-4 w-col-small-4 w-col-tiny-4">
					<div class="nerf_mobile_inst"><span class="text-span_nerf">3 </span> ETH Zurich </div>
				</div>
				<div class="nerf_mobile_col_inst w-col w-col-4 w-col-small-4 w-col-tiny-4">
					<div class="nerf_mobile_inst"><span class="text-span_nerf">4 </span> SenseTime </div>
				</div>
			</div>
			<div class="nerf_authors_list_single nerf_authors_affiliation w-row">
				<div class="w-col w-col-2"><h1 class="nerf_affiliation_v2"> Monash University </h1></div>
				<div class="column w-col w-col-2"><h1 class="nerf_affiliation_v2"> MBZUAI </h1></div>
				<div class="w-col w-col-2"><h1 class="nerf_affiliation_v2"> ETH Zurich </h1></div>
				<div class="w-col w-col-2"><h1 class="nerf_affiliation_v2"> SenseTime </h1></div>
				<div class="w-col w-col-2"><h1 class="nerf_affiliation_v2">Monash University</h1></div>
				<div class="w-col w-col-2"><h1 class="nerf_affiliation_v2">Monash University</h1></div>
				<div class="w-col w-col-2"><h1 class="nerf_affiliation_v2">Monash University</h1></div>
			</div>
			<div class="div-block-10">
				<div class="nerf_equal_v2"><span class="text-span_nerf">*</span><span class="text-span_nerf_star">*</span>Denotes Equal Contribution</div>
			</div>
			<div class="link_column_nerf_v2 w-row">
				<div class="w-col w-col-4 w-col-small-4 w-col-tiny-4">
					<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Cheng_Implicit_Motion_Handling_for_Video_Camouflaged_Object_Detection_CVPR_2022_paper.pdf" class="link-block w-inline-block">
						<img src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cab99df4998decfbf9e218e_paper-01.png" alt="paper" sizes="(max-width: 479px) 12vw, (max-width: 767px) 7vw, (max-width: 991px) 41.8515625px, 56.6953125px" srcset="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cab99df4998decfbf9e218e_paper-01-p-500.png 500w, https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cab99df4998decfbf9e218e_paper-01.png 672w" class="paper_img image-8_nerf">
					</a>
				</div>	
				<div class="w-col w-col-4 w-col-small-4 w-col-tiny-4">
					<a href="paper/[2022][CVPR]VCOD_MoCA-Mask_Chinese.pdf" class="link-block w-inline-block">
						<img src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cab99df4998decfbf9e218e_paper-01.png" alt="中译版" sizes="(max-width: 479px) 12vw, (max-width: 767px) 7vw, (max-width: 991px) 41.8515625px, 56.6953125px" srcset="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cab99df4998decfbf9e218e_paper-01-p-500.png 500w, https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cab99df4998decfbf9e218e_paper-01.png 672w" class="paper_img image-8_nerf">
					</a>
				</div>
				<div class="w-col w-col-4 w-col-small-4 w-col-tiny-4">
					<a href="https://github.com/XuelianCheng/SLT-Net" target="_blank" class="link-block w-inline-block"><img src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cae3b53b42ebb3dd4175a82_68747470733a2f2f7777772e69636f6e66696e6465722e636f6d2f646174612f69636f6e732f6f637469636f6e732f313032342f6d61726b2d6769746875622d3235362e706e67.png" alt="code" class="paper_img image-8 github_icon_nerf_v2"></a>
				</div>
				<div class="column-2 w-col w-col-4 w-col-small-4 w-col-tiny-4">
					<a href="https://drive.google.com/drive/folders/1DEe552YXlcX0qO00MRl21okZupMVsbaZ?usp=sharing" target="_blank" class="link-block w-inline-block"><img src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5e7136849ee3b0a0c6a95151_database.svg" alt="dataset[Google Drive]" class="paper_img image-8_nerf nerf_db_icon"></a>
				</div>
				<div class="column-2 w-col w-col-4 w-col-small-4 w-col-tiny-4">
					<a href="https://drive.google.com/drive/folders/1DEe552YXlcX0qO00MRl21okZupMVsbaZ?usp=sharing" target="_blank" class="link-block w-inline-block"><img src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5e7136849ee3b0a0c6a95151_database.svg" alt="dataset[Baidu Drive]" class="paper_img image-8_nerf nerf_db_icon"></a>
				</div>
			</div>
			<div class="paper_code_nerf w-row">
				<div class="w-col w-col-4 w-col-small-4 w-col-tiny-4">
					<div class="text-block-2"><strong class="bold-text-nerf_v2">Paper</strong>
					</div>
				</div>
				<div class="w-col w-col-4 w-col-small-4 w-col-tiny-4">
					<div class="text-block-2"><strong class="bold-text-nerf_v2">中译版</strong>
					</div>
				</div>
				
				<div class="w-col w-col-4 w-col-small-4 w-col-tiny-4">
					<div class="text-block-2"><strong class="bold-text-nerf_v2">&lt;/Code&gt;</strong>
					</div>
				</div>
				<div class="w-col w-col-4 w-col-small-4 w-col-tiny-4">
					<div class="text-block-2"><strong class="bold-text-nerf_v2">Data[Google Drive]</strong>
					</div>
				</div>
				<div class="w-col w-col-4 w-col-small-4 w-col-tiny-4">
					<div class="text-block-2"><strong class="bold-text-nerf_v2">Data[Baidu Drive]</strong>
					</div>
				</div>
			</div>
		</div>
	</div>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				We propose a new video camouflaged object detection (VCOD) framework that can exploit both short-term dynamics and long-term temporal consistency to detect camouflaged 
				objects from video frames. An essential property of camouflaged objects is that they usually exhibit patterns 
				similar to the background and thus make them hard to identify from still images. Therefore, effectively handling
				temporal dynamics in videos becomes the key for the VCOD task as the camouflaged objects will be noticeable 
				when they move. However, current VCOD methods often leverage homography or optical flows to represent 
				motions, where the detection error may accumulate from both the motion estimation error and the segmentation error.
				On the other hand, our method unifies motion estimation and object segmentation within a single optimization 
				framework. Specifically, we build a dense correlation volume to implicitly capture motions between neighbouring 
				frames and utilize the final segmentation supervision to optimize the implicit motion estimation and segmentation 
				jointly. Furthermore, to enforce temporal consistency within a video sequence, we jointly utilize a spatio-temporal 
				transformer to refine the short-term predictions. Extensive experiments on VCOD benchmarks demonstrate the architectural effectiveness of our approach.
			</td>
		</tr>
	</table>
	<br>

	<center><h1>Overall Pipeline</h1></center>
	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:550px" src="./images/overall.png"/>
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
				The overall pipeline of the SLT-Net. The SLT-Net consists of a short-term detection module and a long-term refinement module.
				The short-term detection module takes a pair of consecutive frames and predicts the camouflaged object mask for the reference 
				frame. The long-term refinement module takes T predictions from the short-term detection module along with their corresponding 
				referenced frames to generate the final predictions.	
				</td>
			</tr>
		</table>
	</center>
	
	<center><h1>Short-term Pipeline</h1></center>
	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:600px" src="./images/short_term.png"/>
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
				The overview of our short-term pipeline. The network first extracts features from the input frames by a transformer encoder,
				then computes a full-range volumetric correspondence between the reference frame I_t and its neighboring frame I_{t+1} to form a correlation
				volume pyramid. A CNN decoder is used to predict the final prediction from the motions captured by the short-term correlation pyramid.
				</td>
			</tr>
		</table>
	</center>

	<center><h1>Long-term Pipeline</h1></center>
	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:600px" src="./images/long_term.png"/>
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
				The overview of the proposed long-term consistency architecture. It formulates the process as a seq-to-seq modeling problem
				and refines the pair-wise predictions with a sequence-to-sequence transformer.
				</td>
			</tr>
		</table>
	</center>

	<br>
	<hr>
	<center><h1>MoCA-Mask Dataset</h1></center>
	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:600px" src="./images/summ.png"/>
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
					Summary for training and test set distribution. Our MoCA-Mask dataset includes 87 video sequences in total, 
					in which 16 sequences were tagged as ''unknow'' (colored in orange). 
					This split is used to validate the sensitivity of different models on novel samples. Zoom-in for details.
				</td>
			</tr>
		</table>
	</center>

	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:600px" src="./images/samples.png"/>
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
					Representative samples from MoCA-Mask. The dataset is quite challenging including diverse scenes, suash as various lighting conditions, 
					i.e, dark and sunny, complex background, camera motions, small ratio of animals and tiny body structures, such as slim torso /limbs.
				</td>
			</tr>
		</table>
	</center>

	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:600px" src="./images/supp.png"/>
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
				Comparison of our proposed network with two top-performing baselines on MoCA-Mask test dataset.
				</td>
			</tr>
		</table>
	</center>

	<br>

	<table align=center width=450px>
		<center><h1>Paper and Supplementary Material</h1></center>
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></a></td>
			<td><span style="font-size:14pt">Xuelian Cheng, Huan Xiong, Deng-Ping Fan, Yiran Zhong, Mehrtash Harandi, Tom Drummond, Zongyuan Ge<br>
				<b>Implicit Motion Handling for Video Camouflaged Object Detection</b><br>
				In CVPR, 2022.<br>
				(hosted on <a href="http://arxiv.org/abs/2203.07363">ArXiv</a>)<br>
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="./resources/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>

	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>
	
	<hr>	
	<br>
	<table align=center width=400>
		<tr>
			<td width=200><center><h1>Visitors</h1></center>
				<!-- <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=uImnlGQbvLcGtZlwHXUkdnb_6p8QDj4V_CPra0MAdRM"></script> -->
				<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=uImnlGQbvLcGtZlwHXUkdnb_6p8QDj4V_CPra0MAdRM&co=1e5882&cmo=5daa5f&cmn=7f313b&ct=ffffff'></script>
			</td>
		</tr>
	</table>

<br>
</body>
</html>